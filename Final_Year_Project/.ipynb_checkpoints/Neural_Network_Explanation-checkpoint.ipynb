{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dhrre\\\\Desktop\\\\DaTAbaSE\\\\GIT stuff\\\\Nueral_Nets\\\\dhr'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "try: \n",
    "    os.chdir('C:/Users/dhrre/Desktop/DaTAbaSE/GIT stuff/Nueral_Nets/dhr')\n",
    "except:\n",
    "    #ranas directory here\n",
    "    print(\"CHANGE DIRECTORY TO AVOID ERROR\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "* We take an activation function as a sigmoidal function $σ(z)$\n",
    "\n",
    "$y(x) = σ(z) ≡ 1/(1+e^{-z})$\n",
    "\n",
    "Where, $z = \\sum_{j} w_{j}x_{j} - b$ for the output layer\n",
    "\n",
    "* We take a quadratic cost function, $C = \\frac{1}{2n} *\\left \\| y(x)-a \\right \\|^2$\n",
    "\n",
    "Where,\n",
    "    * y(x) = target output expected for the given input \"x\"\n",
    "    * a = actual output determined for the given input\n",
    "    * n = number of training inputs given over whom the summation is\n",
    "\n",
    "* We take the input-data and feedforward it into the network and generate an output \n",
    "* By \n",
    "#### Gradient Descent\n",
    "* Our aim is to reduce the cost function output (which is nothing but Mean Squared Error) and thus reduce the error\n",
    "* To reduce the cost function, we are interested in finding the \"gradient descent\" or the \"direction of the point of minima\" and hence we can reduce the error.\n",
    "* So we need to change the weights and biases (denoted as $\\Delta v$) in such a way that changing it, can allow us to reduce cost function (denoted as $\\Delta C$).\n",
    "* We take the gradient descent of the cost function, (denoted as$\\nabla C$) which is nothing but a vector of partial derivatives of \"C\" with respect to weights or biases (\"v\")\n",
    "\n",
    "$\\nabla C$ ≡ {$\\frac{\\partial C}{\\partial v1}$,$\\frac{\\partial C}{\\partial v2}$,$\\frac{\\partial C}{\\partial v3}$ . . .}\n",
    "* Hence, $\\Delta C \\approx \\nabla C \\cdot \\Delta v$\n",
    "* To decrease the cost, we need to ensure that $\\Delta C$ is negative.\n",
    "* For this, we take $\\Delta v = -\\eta\\nabla C$ which makes $\\Delta C \\approx -\\eta\\left \\| \\nabla C  \\right \\|^2$ and therefore always negative.\n",
    "* $\\nabla C$ is computed by finding $\\frac{\\partial C}{\\partial v}$ for all v as shown below:\n",
    "* We know,\n",
    "    * $C = \\frac{1}{2n} *\\left \\| y(x)-a \\right \\|^2$\n",
    "    * $y(x) = σ(z)$\n",
    "    * $z = \\sum_{j} w_{j}x_{j} - b$\n",
    "* Also,\n",
    "\n",
    "<span style=\"font-size:1.2em;\">\n",
    "$\\frac{\\partial C}{\\partial v} = \\frac{\\partial C}{\\partial y(x)}\\cdot\\frac{\\partial y(x)}{\\partial z}\\cdot\\frac{\\partial z}{\\partial v} $\n",
    "$\\therefore   \\frac{\\partial C}{\\partial v} = \\frac{1}{2}*2*y(x)\\cdotσ^{'}(z)\\cdot x$\n",
    "* Luckily, the cost function that we have chosen has a sweet derivative, \n",
    "\n",
    "$σ^{'}(z) = e^{-z}/(1+e^{-z})^2$ which is nothing but $σ(z)(1 - σ(z))$\n",
    "\n",
    "<span style=\"font-size:1.2em;\">\n",
    "$\\therefore   \\frac{\\partial C}{\\partial v} = y(x)\\cdot σ(z)(1 - σ(z)) \\cdot x$\n",
    "* Taking everything in vector form, we get $\\nabla C$\n",
    "#### Error Back Propogation\n",
    "* To update the weights, we use the error back propogation algorithm. There are basically 4 equations that govern the algorithm for weight updation. To understand these, the following convention needs to be understood.\n",
    "\n",
    "<span style=\"font-size:1.5em;\">\n",
    "$\\bigcirc_{k^{th} neuron} \\xrightarrow[]{w^l_{jk}} \\bigcirc_{j^{th} neuron}$\n",
    "\n",
    "$(l - 1)^{th} layer \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; l^{th} layer$\n",
    "\n",
    "* Equations of Error Back Propogation are:-\n",
    "\n",
    "<span style=\"font-size:1.2em;\">\n",
    "$\\delta^l = \\nabla_a C \\bigodot σ^{'}(z^l)$\n",
    "\n",
    "<span style=\"font-size:1.2em;\">\n",
    "$\\delta^l = ((w^{l+1})^T \\cdot \\delta^{l+1}) \\bigodot σ^{'}(z^l) $\n",
    "\n",
    "<span style=\"font-size:1.2em;\">\n",
    "$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$\n",
    "\n",
    "<span style=\"font-size:1.2em;\">\n",
    "$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\cdot \\delta^l_j$\n",
    "\n",
    "* With these equations, we find out the $\\delta$ (error) for each layer and thus the weights are updated according to the formulae\n",
    "#### Stochastic mini-batches\n",
    "* For the purpose of better recognition, a neural network must be fed with a lot of training data. This training data must be random and not ordered. Thus we feed random inputs to our network. Also the weights and biases are randomly assigned for best results. This is a heuristic approach that has been followed.\n",
    "* But doing this for many inputs can be computationally costly if weights have to be evaluated after each input\n",
    "* So we use the concept of using mini-batches that divide the input data into small batches and provide them to the network. The error is calculated over this batch and so are the various derivates involved. This is also a hueristic approach that helps reducing computation and results are approximately the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The  \"\\_\\_init__\" function\n",
    "* The list ``sizes`` contains the number of neurons in the respective layers of the network.\n",
    "* For example, if the list was       [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron.\n",
    "* The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. This means that any value between -1 to 1 can be set as a value of the weights.\n",
    "* Note that the first layer is assumed to be an input layer, and thus biases for those neurons are not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self,sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The  \"feedforward\" function\n",
    "* Returns the output for the network if the given input is \"``a``\"\n",
    "* The \"zip\" function makes a list with elements that are matched sequentially with parameters passed to \"zip\"\n",
    "* For example: zip([a,b,c],[1,2,3]) would return [(a,1),(b,2),(c,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "    return a\n",
    "\n",
    "Network.feedforward = feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"update_mini_batch\" function\n",
    "* Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch.\n",
    "* ``mini_batch`` => list of tuples ``(x, y)``\n",
    "* ``eta`` ========> learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_mini_batch(self, mini_batch, eta):\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "    self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "Network.update_mini_batch = update_mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stochastic Gradient Descent function\n",
    "* Used to train the neural network using mini-batch stochastic gradient descent.\n",
    "* The \"``training_data``\" is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs.\n",
    "* While \"``range``\" returns an array which can be later used to feed values to the \"``for``\" loop, an \"``xrange``\" does not pre-generate the list, but generates each element during execution\n",
    "\n",
    "\n",
    "* ``epochs`` ==========> number of epochs to be run\n",
    "* ``training_data`` ===> the actual training data (From MNIST)\n",
    "* ``test_data`` =======> If \"``test_data``\" is provided then the network will be evaluated against the test data after each epoch,   and partial progress printed out.  This is useful for tracking progress, but slows things down substantially. We have \"``test_data``\" pre-initialized to \"``None``\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    if test_data: n_test = len(test_data)\n",
    "    n = len(training_data)\n",
    "    for j in xrange(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size) ]\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, eta)\n",
    "        if test_data:\n",
    "            print \"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test)\n",
    "        else:\n",
    "            print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "Network.SGD = SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"backprop\" function\n",
    "* Returns a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C.\n",
    "* ``nabla_b`` and ``nabla_w`` are layer-by-layer lists, similar to ``self.biases`` and ``self.weights``.\n",
    "* Note that l = 1 means the last layer of neurons, l = 2 is the second-last layer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(self, x, y):\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    # backward pass\n",
    "    delta = self.cost_derivative(activations[-1], y) * \\\n",
    "        sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    for l in xrange(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)\n",
    "\n",
    "Network.backprop = backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"evaluate\" function\n",
    "* Return the number of test inputs for which the neural network outputs the correct result.\n",
    "* Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(self, test_data):\n",
    "    test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "    return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "Network.evaluate = evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"cost_derivative\" function\n",
    "* Return the vector of partial derivatives $\\partial C_x /\\partial a$ for the output activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_derivative(self, output_activations, y):\n",
    "    return (output_activations-y)\n",
    "\n",
    "Network.cost_derivative = cost_derivative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
